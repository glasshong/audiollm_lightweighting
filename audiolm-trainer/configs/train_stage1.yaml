# Copyright (2024) Tsinghua University, Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

model:
  # paths
  gemma_path: "/data/nota/audiolm-evaluator/audiolm-trainer/gemma-2-2B-4Bit-GPTQ"
  whisper_path: "openai/whisper-large-v3-turbo" 
  beats_path: "/data/nota/BEATs_iter3_plus_AS20K_finetuned_on_AS2M_cpt2.pt" 

  token:  # Use hf token to access gated repositories  

  ckpt: # "model ckpt" # if not "", load model from ckpt for training or evaluation

  freeze_whisper: True
  freeze_beats: True

  # window-level Q-Former
  use_speech_Qformer: True
  freeze_speech_QFormer: False 
  window_level_Qformer: True
  num_speech_query_token: 1
  second_per_window: 0.333333
  second_stride: 0.333333

  speech_gemma_proj_model: ""
  freeze_speech_gemma_proj: False

  # LoRA
  lora: True
  lora_rank: 8
  lora_alpha: 32 
  lora_dropout: 0.1 

  multi_prompt: True
  # prompt_template: "USER: {}\nASSISTANT:"
  prompt_template: "SYSTEM: You are an assistant doing audio captioning and speech recognition. When you transcribe, select the most likely word from the words with similar pronunciations considering the context. If the sentence includes awkward words in terms of the context, replace them with contextual words with similar pronunciation. When you do audio captioning, do not transcribe the words even if you hear speech. Do not repeat the same phrases. You'd better exclude the phrases such as ‘in the background’ unless necessary.\nUSER: {}\nASSISTANT:"
  prompt_path: "prompts/train_prompt.json"
  test_prompt_path: "prompts/test_prompt.json"
  max_txt_len: 300 
  end_sym: "<eos>"  

datasets:
  prefix: "/data/nota/audio_data"
  
  train_ann_path: "/data/nota/boostcamp-7th-nota-hackathon/stage1_split_train_strf_mod_flac.json"
  valid_ann_path: "/data/nota/boostcamp-7th-nota-hackathon/stage1_split_valid_strf_mod_flac.json"
  test_ann_path: "/data/nota/boostcamp-7th-nota-hackathon/stage1_split_test_strf_mod_flac.json"

  whisper_path: "openai/whisper-large-v3-turbo"

run:
  # log & settings
  seed: 77
  output_dir: "outputs_stage1_only"
  evaluate: False # if True, only evaluate model on test data
  exp_name: "gemma2_4bit_2-stage1_13"

  log_freq: 30
  epoch_based: False
  iters_per_epoch: 3000
  accum_grad_iters: 1
  batch_size_train: 8
  batch_size_eval: 8
  num_workers: 64

  device: "cuda"
  use_distributed: False
  gpu: 1
  amp: True 
  world_size: 2
  dist_url: "env://"

  # optimizer & scheduler
  optims:
    max_epoch: 1
    warmup_steps: 0
    warmup_start_lr: 1e-6
    init_lr: 0.000010245
    min_lr: 1e-5
    weight_decay: 0.05
    beta2: 0.999